# -*- coding: utf-8 -*-
"""new_machin_learning.ipynb のコピー

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WmhzgoGfYl8zREY95XaMdpmjle6SKTvk
"""

import pandas as pd
import numpy as np
import random
import tensorflow as tf
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ランダムシードを固定してランダム性をなくす
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.keras.utils.set_random_seed(SEED)
tf.config.experimental.enable_op_determinism()

# データのファイルパス
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップ設定
time_steps = 60

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを作成し、異なる行動が混ざらないようにする
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わったらバッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファが time_steps に達したらシーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # スライディングウィンドウ方式

    return np.array(sequences), np.array(labels)

# 全データを統合し、ラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_train に合わせて y_train をフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTM モデルを構築
    model = Sequential([
        Input(shape=(time_steps, X_train_sequences.shape[2])),
        LSTM(64),
        Dense(32, activation='relu'),
        Dense(len(label_encoder.classes_), activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルをトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列をデータフレーム化
conf_matrix_count_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列をヒートマップで可視化
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

# 混同行列をヒートマップで表示・保存（カウントベース）
plt.figure(figsize=(12, 10))  # 画像サイズを大きく調整
sns.heatmap(conf_matrix_count_df, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,
            annot_kws={"size": 10})  # 数字のフォントサイズ調整
plt.xticks(fontsize=12, rotation=45)  # x軸ラベルサイズと回転を調整
plt.yticks(fontsize=12)  # y軸ラベルサイズ調整
plt.title(f"Cross-Validation Confusion Matrix (Counts) (Time Steps={time_steps})")

# 結果を保存
output_path = os.path.join(output_dir, f"new_conf_matrix_count_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with counts saved to: {output_path}")

import pandas as pd
import numpy as np
import random
import tensorflow as tf
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ランダムシードを固定してランダム性をなくす
SEED = 1234
random.seed(SEED)
np.random.seed(SEED)
tf.keras.utils.set_random_seed(SEED)
tf.config.experimental.enable_op_determinism()

# データのファイルパス
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップ設定
time_steps = 30

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを作成し、異なる行動が混ざらないようにする
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わったらバッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファが time_steps に達したらシーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # スライディングウィンドウ方式

    return np.array(sequences), np.array(labels)

# 全データを統合し、ラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_train に合わせて y_train をフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTM モデルを構築
    model = Sequential([
        Input(shape=(time_steps, X_train_sequences.shape[2])),
        LSTM(64),
        Dense(32, activation='relu'),
        Dense(len(label_encoder.classes_), activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルをトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列をデータフレーム化
conf_matrix_count_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列をヒートマップで可視化
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)
# 混同行列をヒートマップで表示・保存（カウントベース）
plt.figure(figsize=(12, 10))  # 画像サイズを大きく調整
sns.heatmap(conf_matrix_count_df, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,
            annot_kws={"size": 10})  # 数字のフォントサイズ調整
plt.xticks(fontsize=12, rotation=45)  # x軸ラベルサイズと回転を調整
plt.yticks(fontsize=12)  # y軸ラベルサイズ調整
plt.title(f"Cross-Validation Confusion Matrix (Counts) (Time Steps={time_steps})")

# 結果を保存
output_path = os.path.join(output_dir, f"new_conf_matrix_count_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with counts saved to: {output_path}")

import pandas as pd
import numpy as np
import random
import tensorflow as tf
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ランダムシードを固定してランダム性をなくす
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.keras.utils.set_random_seed(SEED)
tf.config.experimental.enable_op_determinism()

# データのファイルパス
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップ設定
time_steps = 90

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを作成し、異なる行動が混ざらないようにする
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わったらバッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファが time_steps に達したらシーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # スライディングウィンドウ方式

    return np.array(sequences), np.array(labels)

# 全データを統合し、ラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_train に合わせて y_train をフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTM モデルを構築
    model = Sequential([
        Input(shape=(time_steps, X_train_sequences.shape[2])),
        LSTM(64),
        Dense(32, activation='relu'),
        Dense(len(label_encoder.classes_), activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルをトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列をデータフレーム化
conf_matrix_count_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列をヒートマップで可視化
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)
# 混同行列をヒートマップで表示・保存（カウントベース）
plt.figure(figsize=(12, 10))  # 画像サイズを大きく調整
sns.heatmap(conf_matrix_count_df, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,
            annot_kws={"size": 10})  # 数字のフォントサイズ調整
plt.xticks(fontsize=12, rotation=45)  # x軸ラベルサイズと回転を調整
plt.yticks(fontsize=12)  # y軸ラベルサイズ調整
plt.title(f"Cross-Validation Confusion Matrix (Counts) (Time Steps={time_steps})")

# 結果を保存
output_path = os.path.join(output_dir, f"new_conf_matrix_count_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with counts saved to: {output_path}")

"""150フレーム

"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os
import tensorflow as tf

# 再現性のためにシードを固定
tf.random.set_seed(42)
np.random.seed(42)

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 120
def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# 全データを統合してラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)  # 形状を固定
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 数値の混同行列をデータフレーム化
conf_matrix_count_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 結果をヒートマップで表示・保存
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)
# 混同行列をヒートマップで表示・保存
plt.figure(figsize=(12, 10))  # 画像サイズを大きく調整
sns.heatmap(conf_matrix_ratio_df, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,
            annot_kws={"size": 10})  # 数字のフォントサイズ調整
plt.xticks(fontsize=12, rotation=45)  # x軸ラベルサイズと回転を調整
plt.yticks(fontsize=12)  # y軸ラベルサイズ調整
sns.heatmap(conf_matrix_count_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix Counts (Time Steps={time_steps})")

output_path = os.path.join(output_dir, f"new_conf_matrix_count_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with counts saved to: {output_path}")

"""１２０フレーム"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt
import os
import tensorflow as tf
from collections import Counter

# 再現性のためにシードを固定
tf.random.set_seed(42)
np.random.seed(42)

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 150
def create_sequences_by_action_debug(X, y, time_steps):
    """
    全てのアクションに対して150フレーム以上連続している場合にシーケンスを生成
    """
    sequences = []
    labels = []
    buffer = []
    last_action = None

    X = X.reset_index(drop=True)
    y = y.reset_index(drop=True)

    for i in range(len(X)):
        action = y.iloc[i]

        # バッファに追加
        if action == last_action or last_action is None:
            buffer.append(X.iloc[i].values)
        else:
            buffer = [X.iloc[i].values]

        # 150フレームに達したらシーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(action)
            buffer.pop(0)

        last_action = action

    return np.array(sequences), np.array(labels)

# 全データを統合してラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])
print("LabelEncoder classes:", label_encoder.classes_)

# 学習時のデータ不均衡を確認
print("Training label distribution:")
train_label_counts = Counter(all_data['action'])
for label, count in train_label_counts.items():
    print(f"{label}: {count}")

total_samples = sum(train_label_counts.values())
hb_count = train_label_counts.get('Head banging', 0)
hb_ratio = hb_count / total_samples * 100 if total_samples > 0 else 0

print(f"\nHead banging samples: {hb_count}")
print(f"Total samples: {total_samples}")
print(f"Head banging ratio: {hb_ratio:.2f}%")

# バーグラフで可視化
plt.figure(figsize=(10, 6))
plt.bar(train_label_counts.keys(), train_label_counts.values(), color='skyblue')
plt.xticks(rotation=45)
plt.title('Training Data Label Distribution')
plt.xlabel('Action Labels')
plt.ylabel('Count')
plt.show()

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action_debug(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action_debug(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング（class_weightなし）
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # ソフトマックスの出力確認
    y_probs = model.predict(X_test_sequences)
    hb_index = label_encoder.transform(['Head banging'])[0]
    hb_probs = y_probs[:, hb_index]

    plt.figure(figsize=(10, 6))
    plt.hist(hb_probs, bins=50, color='salmon', alpha=0.7)
    plt.title('Distribution of Softmax Probabilities for Head banging Class')
    plt.xlabel('Probability')
    plt.ylabel('Frequency')
    plt.show()

    print(f"Mean probability for Head banging: {hb_probs.mean():.4f}")
    print(f"Standard deviation: {hb_probs.std():.4f}")
    print(f"Percentage of predictions with probability < 0.1: {np.sum(hb_probs < 0.1) / len(hb_probs) * 100:.2f}%")

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列を割合に変換
total_conf_matrix_ratio = np.divide(total_conf_matrix.astype('float'),
                                    total_conf_matrix.sum(axis=1)[:, np.newaxis],
                                    out=np.zeros_like(total_conf_matrix, dtype=float),
                                    where=total_conf_matrix.sum(axis=1)[:, np.newaxis] != 0)

# 割合をヒートマップで表示・保存
conf_matrix_ratio_df = pd.DataFrame(total_conf_matrix_ratio, index=label_encoder.classes_, columns=label_encoder.classes_)
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_ratio_df, annot=True, fmt='.2f', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix Ratios (Time Steps={time_steps})")

output_path = os.path.join(output_dir, f"conf_matrix_ratio_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with ratios saved to: {output_path}")

# Head bangingのシーケンス数を最終的に確認
head_banging_train_count = (y_train == label_encoder.transform(['Head banging'])[0]).sum()
head_banging_test_count = (y_test == label_encoder.transform(['Head banging'])[0]).sum()
print(f"Final 'Head banging' count in training data: {head_banging_train_count}")
print(f"Final 'Head banging' count in test data: {head_banging_test_count}")

import pandas as pd

# 元のファイルパスのリスト
file_paths = [
    'Experiment/data/feature/new_feature/features_with_labels_prerna.csv',
    'Experiment/data/feature/new_feature/features_with_labels_umang.csv',
    'Experiment/data/feature/new_feature/features_with_labels_yutaro.csv',
    'Experiment/data/feature/new_feature/features_with_labels_aoi.csv',
    'Experiment/data/feature/new_feature/features_with_labels_nabe.csv'
]

valid_labels = [
    'Attacking',
    'Biting',
    'Eating snacks',
    'Head banging',
    'Sitting quietly',
    'Throwing things',
    'Using phone',
    'Walking'
]

# 各ファイルを処理
for file_path in file_paths:
    try:
        # ファイルの読み込み
        data = pd.read_csv(file_path)

        # 'action'列が存在するか確認
        if 'action' not in data.columns:
            print(f"'action' column not found in {file_path}")
            continue

        # 指定したラベル以外の行を削除
        filtered_data = data[data['action'].isin(valid_labels)]

        # フィルタリング後のデータを保存
        filtered_data.to_csv(file_path, index=False)
        print(f"Filtered and saved: {file_path}")
    except Exception as e:
        print(f"Error processing {file_path}: {e}")

import pandas as pd

# 元のファイルパスのリスト
file_paths = [
    'Experiment/data/feature/g_feature/features_with_labels_prerna.csv',
    'Experiment/data/feature/g_feature/features_with_labels_umang.csv',
    'Experiment/data/feature/g_feature/features_with_labels_yutaro.csv',
    'Experiment/data/feature/g_feature/features_with_labels_aoi.csv',
    'Experiment/data/feature/g_feature/features_with_labels_nabe.csv'
]

valid_labels = [
    'Attacking',
    'Biting',
    'Eating snacks',
    'Head banging',
    'Sitting quietly',
    'Throwing things',
    'Using phone',
    'Walking'
]

# 各ファイルを処理
for file_path in file_paths:
    try:
        # ファイルの読み込み
        data = pd.read_csv(file_path)

        # 'action'列が存在するか確認
        if 'action' not in data.columns:
            print(f"'action' column not found in {file_path}")
            continue

        # 指定したラベル以外の行を削除
        filtered_data = data[data['action'].isin(valid_labels)]

        # フィルタリング後のデータを保存
        filtered_data.to_csv(file_path, index=False)
        print(f"Filtered and saved: {file_path}")
    except Exception as e:
        print(f"Error processing {file_path}: {e}")

import pandas as pd
import os

# ファイルパスのリスト
file_paths = [
    'Experiment/data/feature/new_feature/features_with_labels_prerna.csv',
    'Experiment/data/feature/new_feature/features_with_labels_umang.csv',
    'Experiment/data/feature/new_feature/features_with_labels_yutaro.csv',
    'Experiment/data/feature/new_feature/features_with_labels_aoi.csv',
    'Experiment/data/feature/new_feature/features_with_labels_nabe.csv'
]

# ファイルごとにNaNのチェック
for input_file in file_paths:
    try:
        # ファイルが存在するか確認
        if not os.path.exists(input_file):
            print(f"File not found: {input_file}")
            continue

        # データを読み込み
        print(f"Checking file: {input_file}")
        data = pd.read_csv(input_file)

        # データ全体にNaNがあるか確認
        if data.isnull().values.any():
            print(f"NaN found in file: {input_file}")
            print(data.isnull().sum())  # 列ごとのNaNの数を出力
        else:
            print(f"No NaN values in file: {input_file}")

        # 'action'列に特化してNaNを確認
        if 'action' in data.columns:
            if data['action'].isnull().any():
                print(f"NaN found in 'action' column of file: {input_file}")
            else:
                print(f"No NaN values in 'action' column of file: {input_file}")
        else:
            print(f"'action' column not found in file: {input_file}")
    except Exception as e:
        print(f"An error occurred while checking {input_file}: {e}")

pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd Experiment



"""１フレーム"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/g_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/g_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/g_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/g_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/g_feature/features_with_labels_nabe.csv'
}

# クロスバリデーション用の初期化
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []
total_conf_matrix = None

# クロスバリデーションのループ
for test_person_key in file_paths:
    test_file = file_paths[test_person_key]
    print(f"Using {test_file} as test data...")

    # テストデータを読み込み
    test_data = pd.read_csv(test_file)

    # 残りのデータをトレーニングデータとして読み込み
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # データをLSTM用の3次元データに変換（1フレームのみをタイムステップとして扱う）
    X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
    X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))  # (タイムステップ, 特徴量数)
    model.add(LSTM(64))  # LSTMのユニット数
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred = np.argmax(model.predict(X_test), axis=1)

    # 各スコアを計算
    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, average='weighted')
    recall = recall_score(y_test, y_test_pred, average='weighted')
    f1 = f1_score(y_test, y_test_pred, average='weighted')

    total_accuracy.append(accuracy)
    total_precision.append(precision)
    total_recall.append(recall)
    total_f1.append(f1)

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test, y_test_pred, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列を保存するフォルダを作成
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

# 混同行列をヒートマップで表示・保存
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Cross-Validation Confusion Matrix (1 Frame with LSTM)")

output_path = os.path.join(output_dir, "1_frame.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/n_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/n_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/n_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/n_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/n_feature/features_with_labels_nabe.csv'
}

# クロスバリデーション用の初期化
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []
total_conf_matrix = None

# クロスバリデーションのループ
for test_person_key in file_paths:
    test_file = file_paths[test_person_key]
    print(f"Using {test_file} as test data...")

    # テストデータを読み込み
    test_data = pd.read_csv(test_file)

    # 残りのデータをトレーニングデータとして読み込み
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # データをLSTM用の3次元データに変換（1フレームのみをタイムステップとして扱う）
    X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
    X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))  # (タイムステップ, 特徴量数)
    model.add(LSTM(64))  # LSTMのユニット数
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred = np.argmax(model.predict(X_test), axis=1)

    # 各スコアを計算
    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, average='weighted')
    recall = recall_score(y_test, y_test_pred, average='weighted')
    f1 = f1_score(y_test, y_test_pred, average='weighted')

    total_accuracy.append(accuracy)
    total_precision.append(precision)
    total_recall.append(recall)
    total_f1.append(f1)

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test, y_test_pred, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列を保存するフォルダを作成
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

# 混同行列をヒートマップで表示・保存
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Cross-Validation Confusion Matrix (1 Frame with LSTM)")

output_path = os.path.join(output_dir, "1_frame.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# クロスバリデーション用の初期化
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []
total_conf_matrix = None

# クロスバリデーションのループ
for test_person_key in file_paths:
    test_file = file_paths[test_person_key]
    print(f"Using {test_file} as test data...")

    # テストデータを読み込み
    test_data = pd.read_csv(test_file)

    # 残りのデータをトレーニングデータとして読み込み
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # データをLSTM用の3次元データに変換（1フレームのみをタイムステップとして扱う）
    X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
    X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))  # (タイムステップ, 特徴量数)
    model.add(LSTM(64))  # LSTMのユニット数
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred = np.argmax(model.predict(X_test), axis=1)

    # 各スコアを計算
    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, average='weighted')
    recall = recall_score(y_test, y_test_pred, average='weighted')
    f1 = f1_score(y_test, y_test_pred, average='weighted')

    total_accuracy.append(accuracy)
    total_precision.append(precision)
    total_recall.append(recall)
    total_f1.append(f1)

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test, y_test_pred, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列を保存するフォルダを作成
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

# 混同行列をヒートマップで表示・保存
plt.figure(figsize=(12, 10))  # 画像サイズを大きく調整
sns.heatmap(conf_matrix_ratio_df, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,
            annot_kws={"size": 10})  # 数字のフォントサイズ調整
plt.xticks(fontsize=12, rotation=45)  # x軸ラベルサイズと回転を調整
plt.yticks(fontsize=12)  # y軸ラベルサイズ調整
plt.title("Cross-Validation Confusion Matrix Ratios (Time Steps=1)")
plt.tight_layout()
output_path = os.path.join(output_dir, "14futures_1_frame.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

# Commented out IPython magic to ensure Python compatibility.
# %cd Experiment

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# クロスバリデーション用の初期化
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []
total_conf_matrix = None

# クロスバリデーションのループ
for test_person_key in file_paths:
    test_file = file_paths[test_person_key]
    print(f"Using {test_file} as test data...")

    # テストデータを読み込み
    test_data = pd.read_csv(test_file)

    # 残りのデータをトレーニングデータとして読み込み
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # データをLSTM用の3次元データに変換（1フレームのみをタイムステップとして扱う）
    X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
    X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))  # (タイムステップ, 特徴量数)
    model.add(LSTM(64))  # LSTMのユニット数
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred = np.argmax(model.predict(X_test), axis=1)

    # 各スコアを計算
    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, average='weighted')
    recall = recall_score(y_test, y_test_pred, average='weighted')
    f1 = f1_score(y_test, y_test_pred, average='weighted')

    total_accuracy.append(accuracy)
    total_precision.append(precision)
    total_recall.append(recall)
    total_f1.append(f1)

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test, y_test_pred, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列を割合に変換
conf_matrix_ratio = total_conf_matrix.astype('float') / total_conf_matrix.sum(axis=1)[:, np.newaxis]

# 混同行列をヒートマップで表示・保存
conf_matrix_ratio_df = pd.DataFrame(conf_matrix_ratio, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列を保存するフォルダを作成
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

# 混同行列をヒートマップで表示・保存
plt.figure(figsize=(12, 10))  # 画像サイズを大きく調整
sns.heatmap(conf_matrix_ratio_df, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,
            annot_kws={"size": 10})  # 数字のフォントサイズ調整
plt.xticks(fontsize=12, rotation=45)  # x軸ラベルサイズと回転を調整
plt.yticks(fontsize=12)  # y軸ラベルサイズ調整
plt.title("Cross-Validation Confusion Matrix Ratios (Time Steps=1)")
plt.tight_layout()
output_path = os.path.join(output_dir, "14futures_1_frame_ratios.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with ratios saved to: {output_path}")

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 30
def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# 全データを統合してラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)  # 形状を固定
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列を割合に変換
conf_matrix_ratio = total_conf_matrix.astype('float') / total_conf_matrix.sum(axis=1)[:, np.newaxis]

# 割合をヒートマップで表示・保存
conf_matrix_ratio_df = pd.DataFrame(conf_matrix_ratio, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_ratio_df, annot=True, fmt='.2f', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix Ratios (Time Steps={time_steps})")

output_path = os.path.join(output_dir, f"conf_matrix_ratio_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with ratios saved to: {output_path}")

# Commented out IPython magic to ensure Python compatibility.
# %cd Experiment

# Commented out IPython magic to ensure Python compatibility.
# %cd Experiment
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 90
def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# 全データを統合してラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)  # 形状を固定
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列を割合に変換
conf_matrix_ratio = total_conf_matrix.astype('float') / total_conf_matrix.sum(axis=1)[:, np.newaxis]

# 割合をヒートマップで表示・保存
conf_matrix_ratio_df = pd.DataFrame(conf_matrix_ratio, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(12, 10))  # 画像サイズを大きく調整
sns.heatmap(conf_matrix_ratio_df, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,
            annot_kws={"size": 10})  # 数字のフォントサイズ調整
plt.xticks(fontsize=12, rotation=45)  # x軸ラベルサイズと回転を調整
plt.yticks(fontsize=12)  # y軸ラベルサイズ調整
plt.title(f"Cross-Validation Confusion Matrix Ratios (Time Steps={time_steps})", fontsize=16)
plt.tight_layout()  # レイアウトを自動調整

output_path = os.path.join(output_dir, f"conf_matrix_ratio_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with ratios saved to: {output_path}")

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 90
def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# 全データを統合してラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)  # 形状を固定
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列を割合に変換
conf_matrix_ratio = total_conf_matrix.astype('float') / total_conf_matrix.sum(axis=1)[:, np.newaxis]

# 割合をヒートマップで表示・保存
conf_matrix_ratio_df = pd.DataFrame(conf_matrix_ratio, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(12, 10))  # 画像サイズを大きく調整
sns.heatmap(conf_matrix_ratio_df, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,
            annot_kws={"size": 10})  # 数字のフォントサイズ調整
plt.xticks(fontsize=12, rotation=45)  # x軸ラベルサイズと回転を調整
plt.yticks(fontsize=12)  # y軸ラベルサイズ調整

plt.title(f"Cross-Validation Confusion Matrix Ratios (Time Steps={time_steps})", fontsize=16)
 # レイアウトを自動調整

output_path = os.path.join(output_dir, f"conf_matrix_ratio_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with ratios saved to: {output_path}")

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 30  # 任意のシーケンス長

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences)
    if total_conf_matrix is None:
        total_conf_matrix = conf_matrix
    else:
        total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列を割合に変換
conf_matrix_ratio = total_conf_matrix.astype('float') / total_conf_matrix.sum(axis=1)[:, np.newaxis]

# 割合をヒートマップで表示・保存
conf_matrix_ratio_df = pd.DataFrame(conf_matrix_ratio, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_ratio_df, annot=True, fmt='.2f', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix Ratios (Time Steps={time_steps})")

output_path_ratio = os.path.join(output_dir, f"conf_matrix_ratio_timestep_{time_steps}.png")
plt.savefig(output_path_ratio)
plt.show()

print(f"Confusion matrix with ratios saved to: {output_path_ratio}")

# Commented out IPython magic to ensure Python compatibility.
# %cd Experiment

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 90  # 任意のシーケンス長

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# 全データを統合してラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)  # 形状を固定
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列を割合に変換
conf_matrix_ratio = total_conf_matrix.astype('float') / total_conf_matrix.sum(axis=1)[:, np.newaxis]

# 割合をヒートマップで表示・保存
conf_matrix_ratio_df = pd.DataFrame(conf_matrix_ratio, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_ratio_df, annot=True, fmt='.2f', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix Ratios (Time Steps={time_steps})")

output_path = os.path.join(output_dir, f"conf_matrix_ratio_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix with ratios saved to: {output_path}")

"""レングスシーケンス

"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 0  # 任意のシーケンス長

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences)
    if total_conf_matrix is None:
        total_conf_matrix = conf_matrix
    else:
        total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix (Time Steps={time_steps})")

output_path = os.path.join(output_dir, f"new_conf_matrix_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

"""レングスシーケンス２０"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 20  # 任意のシーケンス長

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences)
    if total_conf_matrix is None:
        total_conf_matrix = conf_matrix
    else:
        total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix (Time Steps={time_steps})")

output_path = os.path.join(output_dir, f"cross_validation_conf_matrix_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

"""レングスシーケンス１０"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/new_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/new_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/new_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/new_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 10  # 任意のシーケンス長

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences)
    if total_conf_matrix is None:
        total_conf_matrix = conf_matrix
    else:
        total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix (Time Steps={time_steps})")

output_path = os.path.join(output_dir, f"cross_validation_conf_matrix_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

"""#1person as training data and 1 person as test data


*   LSTM

#lstm

Baseline


---
"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'data/feature/g_feature/features_with_labels_prerna.csv',
    'person2': 'data/feature/g_feature/features_with_labels_umang.csv',
    'person3': 'data/feature/g_feature/features_with_labels_yutaro.csv',
    'person4': 'data/feature/g_feature/features_with_labels_aoi.csv',
    'person5': 'data/feature/g_feature/features_with_labels_nabe.csv'
}

# クロスバリデーション用の初期化
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []
total_conf_matrix = None

# クロスバリデーションのループ
for test_person_key in file_paths:
    test_file = file_paths[test_person_key]
    print(f"Using {test_file} as test data...")

    # テストデータを読み込み
    test_data = pd.read_csv(test_file)

    # 残りのデータをトレーニングデータとして読み込み
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # LSTM用の3次元にデータをリシェイプ（例: 1タイムステップ）
    X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
    X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred = np.argmax(model.predict(X_test), axis=1)

    # 各スコアを計算
    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, average='weighted')
    recall = recall_score(y_test, y_test_pred, average='weighted')
    f1 = f1_score(y_test, y_test_pred, average='weighted')

    total_accuracy.append(accuracy)
    total_precision.append(precision)
    total_recall.append(recall)
    total_f1.append(f1)

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test, y_test_pred, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列を保存するフォルダを作成
output_dir = "result/all"
os.makedirs(output_dir, exist_ok=True)

# 混同行列をヒートマップで表示・保存
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Cross-Validation Confusion Matrix")

output_path = os.path.join(output_dir, "feature_7_timestep_1.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

"""Baseline+LLMs(feature=10)"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'ex/n_feature/features_with_labels_prerna.csv',
    'person2': 'ex/n_feature/features_with_labels_umang.csv',
    'person3': 'ex/n_feature/features_with_labels_yutaro.csv',
    'person4': 'ex/n_feature/features_with_labels_aoi.csv',
    'person5': 'ex/n_feature/features_with_labels_nabe.csv'
}

# クロスバリデーション用の初期化
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []
total_conf_matrix = None

# クロスバリデーションのループ
for test_person_key in file_paths:
    test_file = file_paths[test_person_key]
    print(f"Using {test_file} as test data...")

    # テストデータを読み込み
    test_data = pd.read_csv(test_file)

    # 残りのデータをトレーニングデータとして読み込み
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # LSTM用の3次元にデータをリシェイプ（例: 1タイムステップ）
    X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
    X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred = np.argmax(model.predict(X_test), axis=1)

    # 各スコアを計算
    accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, average='weighted')
    recall = recall_score(y_test, y_test_pred, average='weighted')
    f1 = f1_score(y_test, y_test_pred, average='weighted')

    total_accuracy.append(accuracy)
    total_precision.append(precision)
    total_recall.append(recall)
    total_f1.append(f1)

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test, y_test_pred, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列を保存するフォルダを作成
output_dir = "cross_validation_confusion_matrix"
os.makedirs(output_dir, exist_ok=True)

# 混同行列をヒートマップで表示・保存
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Cross-Validation Confusion Matrix")

output_path = os.path.join(output_dir, "feature_10_timestep_1.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

"""Baseline+LLMs(feature=14)"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# 4つのファイルをリスト化
file_paths = {
    'person1': 'ex/new_feature/features_with_labels_prerna.csv',
    'person2': 'ex/new_feature/features_with_labels_umang.csv',
    'person3': 'ex/new_feature/features_with_labels_yutaro.csv',
    'person4': 'ex/new_feature/features_with_labels_aoi.csv',
    'person5': 'ex/new_feature/features_with_labels_nabe.csv'
}

# テストファイルを指定
test_person_key = 'person5'  # テストデータとして使用する人物のキー（person1, person2, person3, person4）
test_file = file_paths[test_person_key]
print(f"Using {test_file} as test data...")

# テストデータを読み込み
test_data = pd.read_csv(test_file)

# 残りのデータをトレーニングデータとして読み込み
train_files = [file_paths[key] for key in file_paths if key != test_person_key]
train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

# 特徴量とラベルを分割
X_train = train_data.drop(columns=['action'])
y_train = train_data['action']
X_test = test_data.drop(columns=['action'])
y_test = test_data['action']

# 数値データに変換し、NaNを処理
X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
y_test = y_test[X_test.index]

# LSTM用の3次元にデータをリシェイプ（例: 1タイムステップ）
X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

# ラベルを数値にエンコード
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# LSTMモデルの構築
model = Sequential()
model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(64))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(label_encoder.classes_), activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# モデルのトレーニング
model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

# テストデータで予測
y_test_pred = np.argmax(model.predict(X_test), axis=1)

# 各スコアを計算
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='weighted')
test_recall = recall_score(y_test, y_test_pred, average='weighted')
test_f1 = f1_score(y_test, y_test_pred, average='weighted')

# 結果を表示
print("\nTest Scores:")
print(f'Accuracy: {test_accuracy:.2f}')
print(f'Precision: {test_precision:.2f}')
print(f'Recall: {test_recall:.2f}')
print(f'F1 Score: {test_f1:.2f}')

# 混同行列を計算
conf_matrix = confusion_matrix(y_test, y_test_pred)
conf_matrix_df = pd.DataFrame(conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

# 混同行列を保存するフォルダを作成
output_dir = "new_confusion_matrix"  # フォルダ名を固定
os.makedirs(output_dir, exist_ok=True)

# 混同行列をヒートマップで表示・保存
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"base+LLM")

# ファイル名を "base+LLM.png" に固定
output_path = os.path.join(output_dir, "base+LLM.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

"""base+LLM(クロスバリデーション)

"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# 5つのファイルをリスト化
file_paths = {
    'person1': 'ex/new_feature/features_with_labels_prerna.csv',
    'person2': 'ex/new_feature/features_with_labels_umang.csv',
    'person3': 'ex/new_feature/features_with_labels_yutaro.csv',
    'person4': 'ex/new_feature/features_with_labels_aoi.csv',
    'person5': 'ex/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # LSTM用の3次元にデータをリシェイプ（例: 1タイムステップ）
    X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
    X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred = np.argmax(model.predict(X_test), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test, y_test_pred))
    total_precision.append(precision_score(y_test, y_test_pred, average='weighted'))
    total_recall.append(recall_score(y_test, y_test_pred, average='weighted'))
    total_f1.append(f1_score(y_test, y_test_pred, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test, y_test_pred)
    if total_conf_matrix is None:
        total_conf_matrix = conf_matrix
    else:
        total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "cross_validation_conf_matrix"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Cross-Validation Confusion Matrix")

output_path = os.path.join(output_dir, "cross_validation_base+LLM.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

"""Base＋LLM＋レングスシーケンス(30)"""

pwd

"""Base＋LLM＋レングスシーケンス(60)"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'ex/new_feature/features_with_labels_prerna.csv',
    'person2': 'ex/new_feature/features_with_labels_umang.csv',
    'person3': 'ex/new_feature/features_with_labels_yutaro.csv',
    'person4': 'ex/new_feature/features_with_labels_aoi.csv',
    'person5': 'ex/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 60  # 任意のシーケンス長

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    label_encoder = LabelEncoder()
    y_train = label_encoder.fit_transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences)
    if total_conf_matrix is None:
        total_conf_matrix = conf_matrix
    else:
        total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "cross_validation_confusion_matrix"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix (Time Steps={time_steps})")

output_path = os.path.join(output_dir, f"cross_validation_conf_matrix_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

"""Base＋LLM＋レングスシーケンス(90)"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import os

# ファイルパスをリスト化
file_paths = {
    'person1': 'ex/new_feature/features_with_labels_prerna.csv',
    'person2': 'ex/new_feature/features_with_labels_umang.csv',
    'person3': 'ex/new_feature/features_with_labels_yutaro.csv',
    'person4': 'ex/new_feature/features_with_labels_aoi.csv',
    'person5': 'ex/new_feature/features_with_labels_nabe.csv'
}

# 混同行列とスコアの初期化
total_conf_matrix = None
total_accuracy = []
total_precision = []
total_recall = []
total_f1 = []

# タイムステップを設定
time_steps = 90  # 任意のシーケンス長

def create_sequences_by_action(X, y, time_steps):
    """
    行動ごとにシーケンスを生成し、行動のまたぎを防ぐ
    """
    sequences = []
    labels = []
    current_action = None
    buffer = []

    for i in range(len(X)):
        # 行動が変わった場合、バッファをリセット
        if y.iloc[i] != current_action:
            current_action = y.iloc[i]
            buffer = []

        buffer.append(X.iloc[i].values)

        # バッファがタイムステップに達した場合、シーケンスを作成
        if len(buffer) == time_steps:
            sequences.append(np.array(buffer))
            labels.append(current_action)
            buffer.pop(0)  # シーケンスをスライディング

    return np.array(sequences), np.array(labels)

# 全データを統合してラベルをエンコード
all_data = pd.concat([pd.read_csv(f) for f in file_paths.values()], ignore_index=True)
label_encoder = LabelEncoder()
label_encoder.fit(all_data['action'])

# クロスバリデーションのループ
for test_person_key in file_paths:
    print(f"Using {file_paths[test_person_key]} as test data...")

    # テストデータとトレーニングデータの分割
    test_file = file_paths[test_person_key]
    test_data = pd.read_csv(test_file)
    train_files = [file_paths[key] for key in file_paths if key != test_person_key]
    train_data = pd.concat([pd.read_csv(f) for f in train_files], ignore_index=True)

    # 特徴量とラベルを分割
    X_train = train_data.drop(columns=['action'])
    y_train = train_data['action']
    X_test = test_data.drop(columns=['action'])
    y_test = test_data['action']

    # 数値データに変換し、NaNを処理
    X_train = X_train.apply(pd.to_numeric, errors='coerce').dropna()
    y_train = y_train[X_train.index]  # X_trainに合わせてy_trainをフィルタリング
    X_test = X_test.apply(pd.to_numeric, errors='coerce').dropna()
    y_test = y_test[X_test.index]

    # ラベルを数値にエンコード
    y_train = label_encoder.transform(y_train)
    y_test = label_encoder.transform(y_test)

    # トレーニングデータをシーケンスに変換
    X_train_sequences, y_train_sequences = create_sequences_by_action(
        pd.DataFrame(X_train), pd.Series(y_train), time_steps
    )

    # テストデータをシーケンスに変換
    X_test_sequences, y_test_sequences = create_sequences_by_action(
        pd.DataFrame(X_test), pd.Series(y_test), time_steps
    )

    # シーケンスが空の場合はスキップ
    if len(X_train_sequences) == 0 or len(X_test_sequences) == 0:
        print(f"Skipping {test_person_key} due to insufficient data.")
        continue

    # LSTMモデルの構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))  # タイムステップを入力形状に含む
    model.add(LSTM(64))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデルのトレーニング
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=32, verbose=1)

    # テストデータで予測
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)

    # 各スコアを計算
    total_accuracy.append(accuracy_score(y_test_sequences, y_test_pred_sequences))
    total_precision.append(precision_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_recall.append(recall_score(y_test_sequences, y_test_pred_sequences, average='weighted'))
    total_f1.append(f1_score(y_test_sequences, y_test_pred_sequences, average='weighted'))

    # 混同行列を計算
    conf_matrix = confusion_matrix(y_test_sequences, y_test_pred_sequences, labels=np.arange(len(label_encoder.classes_)))
    if total_conf_matrix is None:
        total_conf_matrix = np.zeros_like(conf_matrix)  # 形状を固定
    total_conf_matrix += conf_matrix

# 平均スコアを計算
avg_accuracy = np.mean(total_accuracy)
avg_precision = np.mean(total_precision)
avg_recall = np.mean(total_recall)
avg_f1 = np.mean(total_f1)

# 結果を表示
print("\nAverage Test Scores across all folds:")
print(f'Accuracy: {avg_accuracy:.2f}')
print(f'Precision: {avg_precision:.2f}')
print(f'Recall: {avg_recall:.2f}')
print(f'F1 Score: {avg_f1:.2f}')

# 統合した混同行列をヒートマップで表示・保存
conf_matrix_df = pd.DataFrame(total_conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

output_dir = "cross_validation_confusion_matrix"
os.makedirs(output_dir, exist_ok=True)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title(f"Cross-Validation Confusion Matrix (Time Steps={time_steps})")

output_path = os.path.join(output_dir, f"cross_validation_conf_matrix_timestep_{time_steps}.png")
plt.savefig(output_path)
plt.show()

print(f"Confusion matrix saved to: {output_path}")

"""ハイパーパラメーターチューニング"""

from tensorflow.keras.optimizers import Adam  # 追加

# ハイパーパラメータ候補
time_steps_candidates = [5, 10]
lstm_units_candidates = [64, 128]
batch_size_candidates = [32, 64]
learning_rate_candidates = [1e-3, 1e-4]

# 全組み合わせを生成
param_grid = list(product(time_steps_candidates, lstm_units_candidates, batch_size_candidates, learning_rate_candidates))

best_f1 = 0
best_params = None

for time_steps, lstm_units, batch_size, learning_rate in param_grid:
    # モデル構築
    model = Sequential()
    model.add(Input(shape=(time_steps, X_train_sequences.shape[2])))
    model.add(LSTM(lstm_units))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(len(label_encoder.classes_), activation='softmax'))

    optimizer = Adam(learning_rate=learning_rate)  # 修正後
    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # モデル訓練
    model.fit(X_train_sequences, y_train_sequences, epochs=20, batch_size=batch_size, verbose=0)

    # 評価
    y_test_pred_sequences = np.argmax(model.predict(X_test_sequences), axis=1)
    f1 = f1_score(y_test_sequences, y_test_pred_sequences, average='weighted')

    if f1 > best_f1:
        best_f1 = f1
        best_params = (time_steps, lstm_units, batch_size, learning_rate)

print("Best F1 Score:", best_f1)
print("Best Params:", best_params)

"""#7:3

randomsammpling Baseline
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 2つのファイルを読み込み、結合する
file1 = pd.read_csv('ex/feature/features_with_labels_left_corrected.csv')
file2 = pd.read_csv('ex/feature/features_with_labels_right_corrected.csv')
combined_data = pd.concat([file1, file2], ignore_index=True)

# 特徴量とラベルを分割
X = combined_data.drop(columns=['action'])
y = combined_data['action']

# 数値データのみに変換し、NaNを処理
X = X.apply(pd.to_numeric, errors='coerce').dropna()
y = y[X.index]  # Xに合わせてyをフィルタリング

# データを70%をトレーニング、30%をテスト用に分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# モデルの学習
model = RandomForestClassifier()
model.fit(X_train, y_train)

# テストデータで予測
y_pred = model.predict(X_test)

# 各スコアを計算
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Test Scores:")
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

# テストデータの混同行列を計算して表示
conf_matrix = confusion_matrix(y_test, y_pred, labels=model.classes_)
conf_matrix_df = pd.DataFrame(conf_matrix, index=model.classes_, columns=model.classes_)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt=".1f", cmap="Blues", xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix on Test Data (70% Train / 30% Test Split)")
plt.savefig('test_confusion_matrix_heatmap.png')
plt.show()

"""randomsammpling Baseline+LLMs

"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 2つのファイルを読み込み、結合する
file1 = pd.read_csv('ex/n_feature/features_with_labels_left_corrected.csv')
file2 = pd.read_csv('ex/n_feature/features_with_labels_right_corrected.csv')
combined_data = pd.concat([file1, file2], ignore_index=True)

# 特徴量とラベルを分割
X = combined_data.drop(columns=['action'])
y = combined_data['action']

# 数値データのみに変換し、NaNを処理
X = X.apply(pd.to_numeric, errors='coerce').dropna()
y = y[X.index]  # Xに合わせてyをフィルタリング

# データを70%をトレーニング、30%をテスト用に分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# モデルの学習
model = RandomForestClassifier()
model.fit(X_train, y_train)

# テストデータで予測
y_pred = model.predict(X_test)

# 各スコアを計算
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Test Scores:")
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

# テストデータの混同行列を計算して表示
conf_matrix = confusion_matrix(y_test, y_pred, labels=model.classes_)
conf_matrix_df = pd.DataFrame(conf_matrix, index=model.classes_, columns=model.classes_)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt=".1f", cmap="Blues", xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix on Test Data (70% Train / 30% Test Split)")
plt.savefig('randomsammpling Baseline+LLMs_heatmap.png')
plt.show()

"""#LSTM

randomsampling
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# データを読み込み、結合
file1 = pd.read_csv('ex/feature/features_with_labels_left_corrected.csv')
file2 = pd.read_csv('ex/feature/features_with_labels_right_corrected.csv')
combined_data = pd.concat([file1, file2], ignore_index=True)

# 特徴量とラベルを分割
X = combined_data.drop(columns=['action'])
y = combined_data['action']

# 数値データに変換し、NaNを処理
X = X.apply(pd.to_numeric, errors='coerce').dropna()
y = y[X.index]  # Xに合わせてyをフィルタリング

# データを70%をトレーニング、30%をテスト用に分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# LSTM用の3次元にデータをリシェイプ（例: 1タイムステップ）
X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

# ラベルを数値にエンコード
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# LSTMモデルの構築
model = Sequential()
model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(64))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(label_encoder.classes_), activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# モデルのトレーニング
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)

# テストデータで予測
y_test_pred = np.argmax(model.predict(X_test), axis=1)

# 各スコアを計算
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='weighted')
test_recall = recall_score(y_test, y_test_pred, average='weighted')
test_f1 = f1_score(y_test, y_test_pred, average='weighted')

print("Test Scores:")
print(f'Accuracy: {test_accuracy:.2f}')
print(f'Precision: {test_precision:.2f}')
print(f'Recall: {test_recall:.2f}')
print(f'F1 Score: {test_f1:.2f}')

# テストデータの混同行列を計算して表示
conf_matrix = confusion_matrix(y_test, y_test_pred)
conf_matrix_df = pd.DataFrame(conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt=".1f", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix on Test Data (LSTM_random Model)")
plt.savefig('lstm_randomsampling.png')
plt.show()

"""Baseline+LLMs


"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# データを読み込み、結合
file1 = pd.read_csv('ex/n_feature/features_with_labels_left_corrected.csv')
file2 = pd.read_csv('ex/n_feature/features_with_labels_right_corrected.csv')
combined_data = pd.concat([file1, file2], ignore_index=True)

# 特徴量とラベルを分割
X = combined_data.drop(columns=['action'])
y = combined_data['action']

# 数値データに変換し、NaNを処理
X = X.apply(pd.to_numeric, errors='coerce').dropna()
y = y[X.index]  # Xに合わせてyをフィルタリング

# データを70%をトレーニング、30%をテスト用に分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# LSTM用の3次元にデータをリシェイプ（例: 1タイムステップ）
X_train = np.expand_dims(X_train.values, axis=1).astype(np.float32)
X_test = np.expand_dims(X_test.values, axis=1).astype(np.float32)

# ラベルを数値にエンコード
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# LSTMモデルの構築
model = Sequential()
model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(64))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(label_encoder.classes_), activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# モデルのトレーニング
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)

# テストデータで予測
y_test_pred = np.argmax(model.predict(X_test), axis=1)

# 各スコアを計算
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='weighted')
test_recall = recall_score(y_test, y_test_pred, average='weighted')
test_f1 = f1_score(y_test, y_test_pred, average='weighted')

print("Test Scores:")
print(f'Accuracy: {test_accuracy:.2f}')
print(f'Precision: {test_precision:.2f}')
print(f'Recall: {test_recall:.2f}')
print(f'F1 Score: {test_f1:.2f}')

# テストデータの混同行列を計算して表示
conf_matrix = confusion_matrix(y_test, y_test_pred)
conf_matrix_df = pd.DataFrame(conf_matrix, index=label_encoder.classes_, columns=label_encoder.classes_)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix_df, annot=True, fmt=".1f", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix on Test Data (LSTM_LLM Model)")
plt.savefig('lstm_Baseline+LLMs.png')
plt.show()